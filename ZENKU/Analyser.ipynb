{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ba739e-b1e3-4824-bdb0-a7514253e664",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e55c142-1a9f-4a66-b11d-f2c57bc0d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from typing import Callable as function\n",
    "class TextColor():\n",
    "        black = '\\033[30m'\n",
    "        red = '\\033[31m'\n",
    "        green = '\\033[32m'\n",
    "        orange = '\\033[33m'\n",
    "        blue = '\\033[34m'\n",
    "        purple = '\\033[35m'\n",
    "        cyan = '\\033[36m'\n",
    "        lightgrey = '\\033[37m'\n",
    "        darkgrey = '\\033[90m'\n",
    "        lightred = '\\033[91m'\n",
    "        lightgreen = '\\033[92m'\n",
    "        yellow = '\\033[93m'\n",
    "        lightblue = '\\033[94m'\n",
    "        pink = '\\033[95m'\n",
    "        lightcyan = '\\033[96m'\n",
    "        reset = '\\033[0m'\n",
    "save_rep = \"E:/DP_database/database\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b1373-695d-432d-a3c4-2bc131927b62",
   "metadata": {},
   "source": [
    "## Ulity Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbdc85-3377-4759-9ca6-543ddd06bd51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Calc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207d551a-4b98-4a59-a891-fdce1adac885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Functions to get true value in case, in case that overflow was noted, in signed values\n",
    "#*\n",
    "def get_overflowed_val(val = 0, bnum = 8):\n",
    "    border = 2**(bnum-1)\n",
    "    if val >= border:\n",
    "        val = val % (border*2)\n",
    "        if val >= border:\n",
    "            val = -border + (val - border)\n",
    "    return val\n",
    "#*\n",
    "# Functions to get p_value, for current correlation vector\n",
    "#*\n",
    "def calc_p_val(corr: float, set_len: int, mode: int = 0) -> float:\n",
    "  t_stat = corr*( ((set_len-2) / (1 - corr**2))**0.5 )\n",
    "  if mode == 0:\n",
    "    df = set_len - 2  # degrees of freedom\n",
    "    p_value = 2 * stats.t.sf(abs(t_stat), df)  # Two-tailed p-value\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91036b9e-cc9b-432d-aef0-d19d8549d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Functions to Hamming weight, or otherwise number of 1 in binary representation vector of target number\n",
    "#*\n",
    "def hamming_weight(x, is_int = True):\n",
    "    bin_val = bin(x)\n",
    "    if not is_int:\n",
    "        return np.count_nonzero(x == 1)\n",
    "    return bin_val.count(\"1\")+bin_val.count(\"-\")\n",
    "#bnum = 8\n",
    "bnum = 16\n",
    "secret_range = 2**(bnum)\n",
    "hw = [hamming_weight(secret_value) for secret_value in range(secret_range)] \n",
    "#np.array([[1,2,3],[5,4,6]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988670cd-f294-494b-bb3a-f58b78d8c10d",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9782c874-4384-4451-bc38-8bfdd37136b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Visualize comparison between told true value graph, and all others\n",
    "#*\n",
    "def compTrueToAll_corrMatrix(corr_map, true_secret:int = 45, xlim:list[int]=None, ylim:list[int]=None, true_last=False, bnum = 8, saveName = None): \n",
    "    global save_rep\n",
    "    secret_corr = corr_map[true_secret]\n",
    "    corr_map = np.array(corr_map)\n",
    "    mask = np.ones(corr_map.shape[0], dtype=bool)\n",
    "    mask[true_secret] = False\n",
    "    all_false_corr = corr_map[mask, :]\n",
    "    indx_ColmnMax = all_false_corr.argmax(axis=0) #Return an vector of indexes of max values in each column of matrix\n",
    "    allFalseVector = np.array([all_false_corr[indx_ColmnMax[i]][i] for i in range(len(indx_ColmnMax))]) #Return a vector of max values for each column in matrix\n",
    "\n",
    "    image = plt.figure()\n",
    "    # Comparison to real correlation\n",
    "    if true_last:\n",
    "        plt.plot(allFalseVector, color='grey', label='all_not_true', linewidth=0.5)\n",
    "        plt.plot(secret_corr, color='red', label=f'secret_val_corr = {true_secret}', linewidth=0.5)\n",
    "    else:\n",
    "        plt.plot(secret_corr, color='red', label=f'secret_val_corr = {true_secret}', linewidth=0.5)\n",
    "        plt.plot(allFalseVector, color='grey', label='all_not_true', linewidth=0.5)\n",
    "    plt.legend(bbox_to_anchor=(0.75, 1.15), ncol=2)\n",
    "    plt.title(\"False_All (Grey) - True (Red)\")\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if saveName is not None:\n",
    "        image.savefig(f'{save_rep}/figures/{saveName}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c512e2-304c-4aac-a3de-c434aa662382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Visualize comparison between told true value graph, and all others\n",
    "#*\n",
    "def compTrueToAll_corrMatrix_pretty(corr_map, true_secret_list:list[int], xlim:list[int]=None, ylim:list[int]=None,\n",
    "                                    true_last=False, bnum = 8, saveName = None): \n",
    "    global save_rep\n",
    "    secret_count = len(true_secret_list)\n",
    "    q_Dim = round(secret_count**0.5)\n",
    "    if q_Dim**2 < secret_count:\n",
    "        q_Dim +=1\n",
    "    plt_dims = (q_Dim, q_Dim)\n",
    "    plt_Max_colms = 4\n",
    "    if q_Dim > plt_Max_colms:\n",
    "        plt_dims = (plt_Max_colms, round(secret_count/plt_Max_colms)+1)\n",
    "    \n",
    "    image = plt.figure(figsize=(40,40))\n",
    "    counter = 1\n",
    "    for true_secret in true_secret_list:\n",
    "        secret_corr = corr_map[true_secret]\n",
    "        corr_map = np.array(corr_map)\n",
    "        mask = np.ones(corr_map.shape[0], dtype=bool)\n",
    "        mask[true_secret] = False\n",
    "        all_false_corr = corr_map[mask, :]\n",
    "        indx_ColmnMax = all_false_corr.argmax(axis=0) #Return an vector of indexes of max values in each column of matrix\n",
    "        allFalseVector = np.array([all_false_corr[indx_ColmnMax[i]][i] for i in range(len(indx_ColmnMax))]) #Return a vector of max values for each column in matrix\n",
    "    \n",
    "        # Comparison to real correlation\n",
    "        if true_last:\n",
    "            plt.subplot(plt_dims[0], plt_dims[0], counter)\n",
    "            plt.plot(allFalseVector, color='grey', label='all_not_true', linewidth=0.5)\n",
    "            plt.subplot(plt_dims[0], plt_dims[0], counter)\n",
    "            plt.plot(secret_corr, color='red', label=f'secret_val_corr = {true_secret}', linewidth=0.5)\n",
    "        else:\n",
    "            plt.subplot(plt_dims[0], plt_dims[0], counter)\n",
    "            plt.plot(secret_corr, color='red', label=f'secret_val_corr = {true_secret}', linewidth=0.5)\n",
    "            plt.subplot(plt_dims[0], plt_dims[0], counter)\n",
    "            plt.plot(allFalseVector, color='grey', label='all_not_true', linewidth=0.5)\n",
    "        #plt.legend(bbox_to_anchor=(0.75, 1.15), ncol=2)\n",
    "        plt.legend(loc='lower center')#bbox_to_anchor=(1.03, 0.035), ncol=2)\n",
    "        plt.title(\"False_All (Grey) - True (Red)\")\n",
    "        plt.xlabel(\"Time Sample\")\n",
    "        plt.ylabel(\"Correlation\")\n",
    "        if xlim is not None:\n",
    "            plt.xlim(xlim)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "        counter +=1\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if saveName is not None:\n",
    "        image.savefig(f'{save_rep}/figures/{saveName}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b69daf9-1146-45f3-ac8d-269e838c523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Plot either one or many graphs into one output figure, with choosen parameters\n",
    "#*\n",
    "def plot_mult(corr_list, _type = \"norm\", mult=True, xlim:list[int]=None, ylim:list[int]=None,\n",
    "              graph_title=\"Multiple Correlation Traces\", ylabel=\"Correlation\", color=None, linewidth=1, borders = None):\n",
    "    global save_rep\n",
    "    if not mult: #len(corr_list.shape) >  1:\n",
    "        corr_list = [corr_list]\n",
    "    image = plt.figure()\n",
    "    plt.title(graph_title)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(ylabel)\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    for corr in corr_list:\n",
    "        if color is None:\n",
    "            plt.plot(corr,linewidth=linewidth)\n",
    "        else:\n",
    "            plt.plot(corr, color=color,linewidth=linewidth)\n",
    "    if borders is not None:\n",
    "        for i in borders:\n",
    "            plt.axvline(x = i, color = 'orange')\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "    os.makedirs('./figures', exist_ok=True)\n",
    "    image.savefig(f'{save_rep}/figures/{_type}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54792345-875e-48d6-80af-f7ed15026014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Dynamic SPA\n",
    "#*\n",
    "E_power = 0\n",
    "def createDiffWave(waves, name, showPlots=True, var_ylim = None):\n",
    "    global save_rep\n",
    "    waves = np.array(waves)\n",
    "    diff_waves = []\n",
    "    square_waves = []\n",
    "    avg_wave = np.mean(waves,axis=0)\n",
    "    \n",
    "    for i in waves:\n",
    "        diff_waves.append(np.subtract(i, avg_wave))\n",
    "    diff_waves = np.array(diff_waves)\n",
    "    diff_avg = np.mean(diff_waves,axis=0)\n",
    "    square_diff =  np.square(diff_waves)\n",
    "    dim1 = len(square_diff)\n",
    "    var = np.sum(square_diff, axis=0) / dim1\n",
    "    if showPlots:\n",
    "        image= plt.figure(figsize=(30,15))\n",
    "        \n",
    "        plt.subplot(2,3,1); plt.plot(np.array(waves[0]))\n",
    "        plt.title(\"Waves[0]\");plt.xlabel(\"Sample\");plt.ylabel(\"Power\")\n",
    "        \n",
    "        plt.subplot(2,3,2); plt.plot(np.array(avg_wave), color=\"green\")\n",
    "        plt.title(\"Avg. wave\");plt.xlabel(\"Sample\");plt.ylabel(\"Power\")\n",
    "        \n",
    "        plt.subplot(2,3,3); plt.plot(np.array(diff_avg), color=\"purple\")\n",
    "        plt.title(\"Difference wave\");plt.xlabel(\"Sample\");plt.ylabel(\"Power\")\n",
    "        \n",
    "        plt.subplot(2,3,4); plt.plot(np.array(var), color=\"orange\")\n",
    "        plt.title(\"Variation wave\");plt.xlabel(\"Sample\");plt.ylabel(\"Power\")\n",
    "        \n",
    "        plt.subplot(2,3,5); plt.plot(np.array(var**0.5), color=\"brown\")\n",
    "        plt.title(\"Standard deviation wave\");plt.xlabel(\"Sample\");plt.ylabel(\"Power\")\n",
    "\n",
    "        #plot_mult(np.array(waves[0]), _type = f\"wave0_{name}\", mult=False, graph_title=\"Waves[0]\", ylabel=\"Power\")\n",
    "        #plot_mult(np.array(avg_wave), _type = f\"avg_wave_{name}\", mult=False, graph_title=\"Avg wave\", ylabel=\"Power\", color=\"green\")\n",
    "        #plot_mult(np.array(diff_avg), _type = f\"difference_wave_{name}\", mult=False, graph_title=\"Difference wave\", ylabel=\"Power\", color=\"purple\")\n",
    "        #plot_mult(np.array(var), _type = f\"variation_{name}\", mult=False, graph_title=\"Variation wave\", ylabel=\"Power\", color=\"orange\", ylim=var_ylim)\n",
    "        #plot_mult(np.array(var**(0.5)), _type = f\"deviaton_{name}\", mult=False, graph_title=\"Standard deviation wave\", ylabel=\"Power\", color=\"brown\", ylim=var_ylim)\n",
    "        plt.show()\n",
    "        plt.close()  # Close the figure to free memory\n",
    "        os.makedirs('./figures', exist_ok=True)\n",
    "        image.savefig(f'{save_rep}/figures/{name}_waves_analysis.png')\n",
    "    global E_power\n",
    "    E_power = np.sum(abs(np.array(avg_wave)))/len(avg_wave)\n",
    "    print(f\"Average power: {E_power}\")\n",
    "    return diff_avg, avg_wave, var, waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7424289-cc57-4303-9842-eafe9b25ce43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average power: 20.6\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#avg_wave = [10, -20, 5, -8, 11, 3, -9, 15, -52, 73]\n",
    "##E_power = np.sum(np.absolute(np.array(avg_wave)))/len(avg_wave)\n",
    "#E_power = np.sum(abs(np.array(avg_wave)))/len(avg_wave)\n",
    "#print(f\"Average power: {E_power}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a263125-33a6-4479-bdc8-7973362dbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Plot graphs, of resulting peak-filtering, alongside graph of it's indexes (to which peaks belongs)\n",
    "#*\n",
    "def show_extraction_method_info(vals_locMaxVector: list[int], indx_ColmnMax: list[int], method_name: str = f'xth-level degree', color: str = \"blue\"):\n",
    "    global save_rep\n",
    "    fig = plt.figure()\n",
    "    plt.plot(vals_locMaxVector, color=color)\n",
    "    plt.title(f'{method_name} Local maxims graph')\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"Secret Value\")\n",
    "    plt.show()\n",
    "    fig.savefig(f'{save_rep}/figures/get_weights/methods/{method_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    indx_localMax = []\n",
    "    for i in range(len(vals_locMaxVector)):\n",
    "        val_toAppend = 0\n",
    "        if vals_locMaxVector[i] > 0:\n",
    "            val_toAppend = indx_ColmnMax[i]\n",
    "        indx_localMax.append(val_toAppend)\n",
    "    local_maxims = [int(indx_ColmnMax[i]) for i in range(len(indx_ColmnMax)) if vals_locMaxVector[i] > 0]\n",
    "    uniques_set = [ x for i, x in enumerate(local_maxims) if x not in local_maxims[:i]]\n",
    "    print(f\"Number of uniques: {len(uniques_set)}\")\n",
    "    print(f\"Number of local maxims: {len(local_maxims)}\")\n",
    "    print(f\"Uniquess: {TextColor.pink}{uniques_set}{TextColor.reset}\")\n",
    "    print(f\"Local maxims: {TextColor.orange}{local_maxims}{TextColor.reset}\")\n",
    "    fig = plt.figure()\n",
    "    plt.plot(indx_localMax, color=color)\n",
    "    plt.title(f\"{method_name} indexes graph\")\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"Index\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552224d7-48b5-4119-a014-d3c6f09704f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "intType = np.uint16\n",
    "if intType == np.uint16:\n",
    "    print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a236e-ecb9-4548-ba4b-f6d7658f55e2",
   "metadata": {},
   "source": [
    "### Intermediate Values of multiplication, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc2caa-7cbc-47a8-a097-85f28613bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Case Functions For Calculation Of Intermediates And General Setup\n",
    "#*\n",
    "\n",
    "import struct\n",
    "endian = 'big'\n",
    "intType = np.uint16\n",
    "value_parts_count = 1\n",
    "\n",
    "#*\n",
    "# Functions to Hamming weight, or otherwise number of 1 in binary representation vector of target number\n",
    "#*\n",
    "def hamming_weight(x, is_int = True):\n",
    "    bin_val = bin(x)\n",
    "    if not is_int:\n",
    "        return np.count_nonzero(x == 1)\n",
    "    return bin_val.count(\"1\")+bin_val.count(\"-\")\n",
    "\n",
    "\n",
    "def int16_to_bites(val: int):\n",
    "    global endian\n",
    "    global intType\n",
    "    # Ensure value is in correct signed 16-bit range\n",
    "    val = np.array(val).astype(intType)  # handles overflow like C\n",
    "    # Pack as signed short\n",
    "    data_type = 'h'\n",
    "    if intType == np.uint16:\n",
    "        data_type = 'H'\n",
    "    byte_rep = struct.pack(f'<{data_type}' if endian == 'little' else f'>{data_type}', val)\n",
    "    byte_list = [f\"{byte:08b}\" for byte in byte_rep]\n",
    "    bit_list = []\n",
    "    for byte in byte_list:\n",
    "        for bit in byte:\n",
    "            bit_list.append(np.int16(bit))\n",
    "    return np.array(bit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486e36c-559d-4495-8000-ed1a1dbab35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Return vector of resulting intermediate values\n",
    "#*\n",
    "def Abs_8bit_intermediateVal(secret_value: int, known_input: list[int]): # Abs_8-bit\n",
    "    H = np.int32(known_input *  secret_value) % 256\n",
    "    return np.array(H)\n",
    "def Abs_32bit_intermediateVal(secret_value: int, known_input: list[int]):\n",
    "    H = np.int32(known_input *  secret_value)\n",
    "    return np.array(H)\n",
    "def HW_8bit_intermediateVal(secret_value: int, known_input: list[int]):\n",
    "    real_values = np.array(np.int32(known_input *  secret_value) % 256) #global hw\n",
    "    #In case we are calculating for signed values, cast real values as that type, because HW of signed and unsigned are different\n",
    "    global intType\n",
    "    if intType == np.int16:\n",
    "        real_values = real_values.astype(intType)\n",
    "    H = [hamming_weight(real_values[i]) for i in range(len(known_input))]\n",
    "    return np.array(H)\n",
    "def HW_32bit_intermediateVal(secret_value: int, known_input: list[int]):\n",
    "    real_values = np.array(np.int32(known_input *  secret_value))\n",
    "    global intType\n",
    "    if intType == np.int16:\n",
    "        real_values = real_values.astype(intType)\n",
    "    H = [hamming_weight(real_values[i]) for i in range(len(known_input))]\n",
    "    return np.array(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abb387-85f7-42ee-b8b2-eb1453affa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variaton of Stochastic leakage models\n",
    "def HW_16bit_intermediateVal_BitePartsSeparated(secret_value: int, known_input: list[int]):\n",
    "    global value_parts_count\n",
    "    H = []\n",
    "    lenght = len(known_input)\n",
    "    parts = int(16 / value_parts_count)\n",
    "    mask = np.uint16((1 << parts) - 1)\n",
    "    real_values = np.array([np.uint16(known_input[i] *  secret_value) for i in range(lenght)])\n",
    "    #In case we are calculating for signed values, cast real values as that type, because HW of signed and unsigned are different\n",
    "    global intType\n",
    "    if intType == np.int16:\n",
    "        real_values = real_values.astype(intType)\n",
    "        \n",
    "    for i in range(value_parts_count):\n",
    "        #print(val)\n",
    "        H.append(np.array(   [hamming_weight(real_values[i] & mask) for i in range(lenght)]   ))\n",
    "        mask = mask << parts\n",
    "    #H = np.flip(np.array(H), axis=0).T\n",
    "    return np.array(H)\n",
    "def HW_16bit_intermediateVal_BitePartsSeparated_SignedSpecial(secret_value: int, known_input: list[int]):\n",
    "    global value_parts_count\n",
    "    H = []\n",
    "    lenght = len(known_input)\n",
    "    offset = int(16 / value_parts_count)\n",
    "    global intType\n",
    "    real_values = (np.array(known_input) *  secret_value).astype(intType)\n",
    "    bite_lists = []\n",
    "    for value in real_values:\n",
    "        binVal = bin(value)\n",
    "        binVal = binVal.replace('0b','')\n",
    "        binVal = binVal.replace('-','1')\n",
    "        bite_string = np.zeros(16, dtype=np.uint16)\n",
    "        bite_string[bite_string.shape[0]-len(binVal):] = np.array([np.uint16(bit) for bit in binVal])\n",
    "        bite_lists.append(bite_string)\n",
    "    bite_lists = np.array(bite_lists)\n",
    "    H = [ np.sum(bite_lists[:, offset*iteration:offset*(iteration+1)], axis=1) for iteration in range(value_parts_count)]\n",
    "    return np.array(H)#np.flip(np.array(H), axis=0).T\n",
    "def HW_16bit_intermediateVal_BitePartsSeparated_C_Endian(secret_value: int, known_input: list[int]):\n",
    "    global value_parts_count \n",
    "    lenght = len(known_input)\n",
    "    H = []\n",
    "    offset = int(16 / value_parts_count)\n",
    "    global intType\n",
    "    real_values = (np.array(known_input) *  secret_value).astype(intType)\n",
    "    #print(real_values)\n",
    "    bite_lists = np.array([int16_to_bites(real_value) for real_value in real_values])\n",
    "    start = 0\n",
    "    H = [ np.sum(bite_lists[:, offset*iteration:offset*(iteration+1)], axis=1) for iteration in range(value_parts_count)]\n",
    "    return np.array(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "951efcbe-3846-40b7-9413-7499fda1b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variaton of Stochastic leakage models\n",
    "def HW_16bit_intermediateVal_BitePartsHWOffsetMult(secret_value: int, known_input: list[int]):\n",
    "    global value_parts_count \n",
    "    lenght = len(known_input)\n",
    "    H = np.ones(lenght)\n",
    "    parts = int(16 / value_parts_count)\n",
    "    mask = np.uint16((1 << parts) - 1)\n",
    "    real_values = np.array([np.uint16(known_input[i] *  secret_value) for i in range(lenght)])\n",
    "    \n",
    "    #In case we are calculating for signed values, cast real values as that type, because HW of signed and unsigned are different\n",
    "    global intType\n",
    "    if intType == np.int16:\n",
    "        real_values = real_values.astype(intType)\n",
    "        \n",
    "    for i in range(value_parts_count):\n",
    "        H = np.multiply( H, np.array([hamming_weight(real_values[i] & mask) for i in range(lenght)]) +1)\n",
    "        mask = mask << parts\n",
    "    #H = np.flip(np.array(H), axis=0).T\n",
    "    return np.array(H)\n",
    "def HW_16bit_intermediateVal_BitePartsHWSquared(secret_value: int, known_input: list[int]):\n",
    "    global value_parts_count \n",
    "    lenght = len(known_input)\n",
    "    H = np.zeros(lenght)\n",
    "    parts = int(16 / value_parts_count)\n",
    "    mask = np.uint16((1 << parts) - 1)\n",
    "    real_values = np.array([np.uint16(known_input[i] *  secret_value) for i in range(lenght)])\n",
    "    \n",
    "    #In case we are calculating for signed values, cast real values as that type, because HW of signed and unsigned are different\n",
    "    global intType\n",
    "    if intType == np.int16:\n",
    "        real_values = real_values.astype(intType)\n",
    "        \n",
    "    for i in range(value_parts_count):\n",
    "        H += np.array([hamming_weight(real_values[i] & mask) for i in range(lenght)]) ** 2\n",
    "        mask = mask << parts\n",
    "    #H = np.flip(np.array(H), axis=0).T\n",
    "    return np.array(H)\n",
    "\n",
    "\n",
    "leakage_models_funcs = np.array([Abs_8bit_intermediateVal, Abs_32bit_intermediateVal, HW_8bit_intermediateVal, HW_32bit_intermediateVal,\n",
    "                                 HW_16bit_intermediateVal_BitePartsHWOffsetMult,\n",
    "                                 HW_16bit_intermediateVal_BitePartsHWSquared,\n",
    "                                HW_16bit_intermediateVal_BitePartsSeparated,\n",
    "                                HW_16bit_intermediateVal_BitePartsSeparated_SignedSpecial,\n",
    "                                HW_16bit_intermediateVal_BitePartsSeparated_C_Endian])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e4190-74f3-4845-aae5-6f11872471d3",
   "metadata": {},
   "source": [
    "### CPA calc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369dd54-c349-4fd7-8882-9a584beb704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(H):\n",
    "    H_mean = np.mean(H)\n",
    "    h_diff = H - H_mean\n",
    "    qsum_H = np.sum(np.square(h_diff))\n",
    "    root_q_sum_H = (qsum_H ** 0.5)\n",
    "    return H_mean, h_diff, qsum_H, root_q_sum_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be0b3c7-fec4-4271-b9e2-63a64a8d9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_parts_count = 1\n",
    "correlation_JumpType = False\n",
    "variance_weighting = False\n",
    "participantType = np.int8\n",
    "\"\"\"\n",
    "    General Correlation Power Analysis (CPA) attack.\n",
    "\n",
    "    Parameters:\n",
    "        known_input: List of inputs (e.g., random list of numbers from 0 to 256)\n",
    "        hyp_leakage_calc: Function that takes secret_value and known_input and returns a leakage model\n",
    "        waves: Power traces (2D np.array)\n",
    "        ith_weight: Currently unused, reserved for future indexing\n",
    "        calc_p_value: Whether to calculate p-values for correlation results\n",
    "        bnum: Bit width of the guessed secret (usually 8)\n",
    "    \n",
    "    Returns:\n",
    "        corr_all_Tsamples: All correlation values\n",
    "        p_val_all_Tsamples: All p-values (if calc_p_value is True)\n",
    "        clean_Mcorr: Clean matrix of max correlations (stochastic case)\n",
    "        clean_weighted_Mcorr: Weighted correlation matrix (stochastic case)\n",
    "        count_weight_Mcorr: Distribution of max picks (stochastic case)\n",
    "\"\"\"\n",
    "def general_CPA(known_input : list[int],\n",
    "                hyp_leakage_cacl: function,\n",
    "                waves = np.array([]),\n",
    "                ith_weight: int = 0,\n",
    "                calc_p_value: bool = False,\n",
    "                bnum:int=8): # Vanilla ANN CPA, for one time_sample a\n",
    "    # Get Dimensions\n",
    "    n_traces, trace_len = len(known_input), len(waves[0])\n",
    "    # Calculating statistics for traces in waves set\n",
    "    qsum_L_list, root_qsum_L_list, l_diff_list = [], [], []\n",
    "    for time_sample in range(trace_len):\n",
    "      # Calculation preparations: target sets extraction\n",
    "      L = waves[:,time_sample]\n",
    "      # Calculation\n",
    "      L_mean, l_diff, qsum_L, root_q_sum_L = get_statistics(L)\n",
    "      # Tidy up\n",
    "      qsum_L_list.append(qsum_L)\n",
    "      root_qsum_L_list.append(root_q_sum_L)\n",
    "      l_diff_list.append(l_diff)\n",
    "\n",
    "\n",
    "    # Preparational calculations, for case of quantization\n",
    "    corr_all_Tsamples, p_val_all_Tsamples = [], []\n",
    "    secret_range = 2**(bnum)\n",
    "    global participantType\n",
    "    converted_known_input = known_input.astype(participantType)\n",
    "    for secret_raw_value in trange(secret_range, desc='Calculating Correlations for the Secret-Key: '): # For current WeightHypothesis do\n",
    "        secret_value = np.uint8(secret_raw_value).astype(participantType)\n",
    "        H_bytes = hyp_leakage_cacl(secret_value=secret_value, known_input=converted_known_input)\n",
    "        corr_Tsamples = []\n",
    "        if len(H_bytes.shape) < 2:\n",
    "            H_mean, h_diff, qsum_H, root_q_sum_H = get_statistics(H_bytes)\n",
    "    \n",
    "            # For current WeightHypothesis, create an Correlation, this vector needs to be created for each time sample:\n",
    "            corr_Tsamples = []\n",
    "            for time_sample in range(trace_len):\n",
    "                # Calculation preparations: target sets extraction\n",
    "                l_diff = l_diff_list[time_sample]\n",
    "                sum_HL = np.sum(h_diff * l_diff)  # Dot product\n",
    "                # Calculation\n",
    "                divider = root_q_sum_H * root_qsum_L_list[time_sample]\n",
    "                corr = sum_HL / divider if divider != 0 else 0\n",
    "                # Tidy up\n",
    "                corr_Tsamples.append(corr)\n",
    "        else: #If leakage_model is stochastic (byte-divided) then resulting correlation needs to be differently handled\n",
    "            H_mean, h_diff, qsum_H, root_q_sum_H = [], [], [], []\n",
    "            for H in H_bytes:\n",
    "                mean, diff, qsum, root_q_sum = get_statistics(H)\n",
    "                H_mean.append(mean); h_diff.append(diff); qsum_H.append(qsum); root_q_sum_H.append(root_q_sum)\n",
    "            byte_count = len(H_bytes)\n",
    "            importance = 1 / byte_count\n",
    "            global value_parts_count\n",
    "            if value_parts_count != byte_count:\n",
    "                print(\"Value parts is not in line with variable byte_count\")\n",
    "            jump = 1\n",
    "            global correlation_JumpType\n",
    "            for time_sample in range(trace_len-(byte_count*jump*correlation_JumpType)):\n",
    "                bytes_corrs = np.zeros(byte_count)\n",
    "                # Calculation preparations: target sets extraction\n",
    "                corr = 0\n",
    "                for i in range(byte_count):\n",
    "                    _offset = i*jump*correlation_JumpType\n",
    "                    #corr += calcCorr(h_diff[i], l_diff_list[time_sample+i], root_q_sum_H[i], root_qsum_L_list[time_sample+i])\n",
    "                    sum_HL = np.sum(h_diff[i] * l_diff_list[time_sample+_offset]) \n",
    "                    divider = root_q_sum_H[i] * root_qsum_L_list[time_sample+_offset] # Correlation calculation\n",
    "                    bytes_corrs[i] = abs(sum_HL / divider if divider != 0 else 0)#*importance\n",
    "                corr_Tsamples.append(bytes_corrs)\n",
    "        \n",
    "        \n",
    "        p_val_Tsamples = []\n",
    "        if calc_p_value:\n",
    "            corr = np.sum(bytes_corrs)\n",
    "            lenght = n_traces-2\n",
    "            for corr in corr_Tsamples:\n",
    "                t_stat = corr*( (lenght / (1 - corr**2))**0.5 )\n",
    "                df = n_traces - 2  # degrees of freedom\n",
    "                p_value = 2 * stats.t.sf(abs(t_stat), df)  # Two-tailed p-value\n",
    "                # Tidy up\n",
    "                p_val_Tsamples.append(p_value)\n",
    "            p_val_all_Tsamples.append(p_val_Tsamples)\n",
    "        corr_all_Tsamples.append(corr_Tsamples)\n",
    "    \n",
    "    # Post-Processing\n",
    "    corr_all_Tsamples = np.array(corr_all_Tsamples)\n",
    "    global variance_weighting\n",
    "    if variance_weighting:\n",
    "        weightH_Var = np.std(corr_all_Tsamples, axis=0)\n",
    "        for weightH in range(corr_all_Tsamples.shape[0]):\n",
    "            corr_all_Tsamples[weightH] *= weightH_Var\n",
    "    if len(corr_all_Tsamples.shape) > 2: \n",
    "        #Prepare containers for all results variations\n",
    "        winners = np.argmax(corr_all_Tsamples,axis=0) #get indexes/weight_h of max correlations, at place of j-th byte\n",
    "        clean_Mcorr = np.zeros(corr_all_Tsamples.shape) #sum of only relevant maxims\n",
    "        clean_weighted_Mcorr = np.zeros(corr_all_Tsamples.shape) #weighted sum of maxims (weighted by corresponding count of given weight_h in byte parts for target time sample)\n",
    "        count_weight_Mcorr = np.zeros(corr_all_Tsamples.shape[0:2]) #common max pick\n",
    "        count_weight_Mcorr_origin = np.zeros(corr_all_Tsamples.shape[0:2]) #common max pick\n",
    "        no_weight_h = corr_all_Tsamples.shape[0] #number of weight_hs\n",
    "\n",
    "        #Do maxim slicing of correlation_matrix, and post processings\n",
    "        for i in range(corr_all_Tsamples.shape[1]): #For i-th sample in trace\n",
    "            for j in range(corr_all_Tsamples.shape[2]):#For j-th bite in leakage models result\n",
    "                w = winners[i,j] #pick winning weight_h for chosen byte-representation-part\n",
    "                clean_Mcorr[w,i,j] = corr_all_Tsamples[w,i,j]#and replace 0 correlation with winning correlation\n",
    "            c = np.bincount(winners[i], minlength=corr_all_Tsamples.shape[0])#get counts of each maxims in current time sample\n",
    "            max_count = c[np.argmax(c)] #get greates count\n",
    "            #count_weight_Mcorr[:,i] = c# / no_weight_h\n",
    "            count_weight_Mcorr[c==max_count,i] = np.sum(clean_Mcorr[c==max_count,i], axis=1)\n",
    "            count_weight_Mcorr_origin[c==max_count,i] = np.sum(corr_all_Tsamples[c==max_count,i], axis=1)\n",
    "            for j in range(corr_all_Tsamples.shape[2]):#For j-th bite \n",
    "                clean_weighted_Mcorr[c!=0, i, j] = clean_Mcorr[c!=0, i,j] * c[c!=0] / no_weight_h\n",
    "                \n",
    "        return np.sum(corr_all_Tsamples, axis=2), np.array(p_val_all_Tsamples), np.sum(clean_Mcorr, axis=2), np.sum(clean_weighted_Mcorr, axis=2), count_weight_Mcorr_origin, count_weight_Mcorr\n",
    "    else:\n",
    "        return corr_all_Tsamples, np.array(p_val_all_Tsamples), None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf6a01a-12cd-4459-b8ee-5975851c68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(num):\n",
    "    if np.int8(np.int32(num)) < 0:\n",
    "        return 0\n",
    "    return num\n",
    "#*\n",
    "# Agregate Functions: For calculation of correlation above, waves dataset, with hypotetical created traces based of known_input and knwon weghts\n",
    "#*\n",
    "def weights_verification_CPA(known_input : list[int], known_weights : list[int],\n",
    "                             hyp_leakage_cacl: function, waves = np.array([])\n",
    "                             ,calc_p_value: bool = False, bnum:int=8): # Vanilla ANN CPA, for one time_sample a, PS: send only limited size of waves\n",
    "    # Verification of parameters:\n",
    "    n_traces = len(known_input)\n",
    "    trace_len = len(waves[0])\n",
    "\n",
    "    # Calculating statistics for traces in waves set\n",
    "    qsum_L_list = []\n",
    "    root_qsum_L_list = []\n",
    "    l_diff_list = []\n",
    "    for time_sample in range(trace_len):\n",
    "      # Calculation preparations: target sets extraction\n",
    "      L = waves[:,time_sample]\n",
    "      L_mean = np.mean(L)\n",
    "      # Calculation\n",
    "      l_diff = L - L_mean  # Vectorized\n",
    "      qsum_L = np.sum(np.square(l_diff))  # Sum of squared differences\n",
    "      # Tidy up\n",
    "      qsum_L_list.append(qsum_L)\n",
    "      root_qsum_L_list.append(qsum_L ** 0.5)\n",
    "      l_diff_list.append(l_diff)\n",
    "\n",
    "\n",
    "    # Preparational calculations, for case of quantization\n",
    "    corr_all_Tsamples = []\n",
    "    p_val_all_Tsamples = []\n",
    "    secret_range = 2**(bnum)\n",
    "\n",
    "    global input_size\n",
    "    H = np.zeros(n_traces, dtype=np.uint8)\n",
    "    intermediates = np.zeros(n_traces, dtype=np.uint8)\n",
    "    for secret_value in known_weights:\n",
    "        intermediates_tmp = np.array(known_input) * secret_value\n",
    "        intermediates = np.add(intermediates, intermediates_tmp) % 256\n",
    "        #print(f\"Secret: {secret_value}  input:{known_input[0]}  res:{intermediates[0]}\")\n",
    "    #ReLU\n",
    "    #print(intermediates)\n",
    "    intermediates = np.array([ReLU(i) for i in intermediates])\n",
    "    H = hyp_leakage_cacl(1, intermediates)\n",
    "    H_mean, h_diff, qsum_H, root_q_sum_H = get_statistics(H)    \n",
    "\n",
    "    # For current WeightHypothesis, create an Correlation, this vector needs to be created for each time sample:\n",
    "    corr_Tsamples = []\n",
    "    for time_sample in range(trace_len):\n",
    "        # Calculation preparations: target sets extraction\n",
    "        l_diff = l_diff_list[time_sample]\n",
    "        sum_HL = np.sum(h_diff * l_diff)  # Dot product\n",
    "        # Calculation\n",
    "        divider = root_q_sum_H * root_qsum_L_list[time_sample]\n",
    "        corr = sum_HL / divider if divider != 0 else 0\n",
    "        # Tidy up\n",
    "        corr_Tsamples.append(corr)\n",
    "    corr_all_Tsamples.append(corr_Tsamples)\n",
    "    \n",
    "    p_val_Tsamples = []\n",
    "    if calc_p_value:\n",
    "        lenght = n_traces-2\n",
    "        for corr in corr_Tsamples:\n",
    "            t_stat = corr*( (lenght / (1 - corr**2))**0.5 )\n",
    "            df = n_traces - 2  # degrees of freedom\n",
    "            p_value = 2 * stats.t.sf(abs(t_stat), df)  # Two-tailed p-value\n",
    "            # Tidy up\n",
    "            p_val_Tsamples.append(p_value)\n",
    "        p_val_all_Tsamples.append(p_val_Tsamples)\n",
    "        \n",
    "    return np.array(corr_all_Tsamples), np.array(p_val_all_Tsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5042d1f-24f5-489f-9937-e32797bbe8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int8(127)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.int8(127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae765f9-9fad-441a-a09b-97a707fdec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Differentiade boxed function of ANN's_Mult_CPA, \n",
    "#*\n",
    "\n",
    "def Abs_8bit_ANN_CPA(known_input : list[int], waves = np.array([]), n_traces: int = None, ith_weight: int = 0, calc_p_value: bool = False): # Vanilla ANN CPA, for one time_sample a\n",
    "    corr_all_Tsamples, p_val_all_Tsamples = general_CPA(known_input=known_input, hyp_leakage_cacl=Abs_8bit_intermediateVal,\n",
    "                                    waves=waves, ith_weight=ith_weight, calc_p_value=calc_p_value)\n",
    "    return corr_all_Tsamples, p_val_all_Tsamples\n",
    "\n",
    "def Abs_32bit_ANN_CPA(known_input : list[int], waves = np.array([]), n_traces: int = None, ith_weight: int = 0, calc_p_value: bool = False): # Vanilla ANN CPA, for one time_sample a\n",
    "    corr_all_Tsamples, p_val_all_Tsamples = general_CPA(known_input=known_input, hyp_leakage_cacl=Abs_32bit_intermediateVal,\n",
    "                                    waves=waves, ith_weight=ith_weight, calc_p_value=calc_p_value)\n",
    "    return corr_all_Tsamples, p_val_all_Tsamples\n",
    "\n",
    "def HW_8bit_ANN_CPA(known_input : list[int], waves = np.array([]), n_traces: int = None, ith_weight: int = 0, calc_p_value: bool = False): # Vanilla ANN CPA, for one time_sample a\n",
    "    corr_all_Tsamples, p_val_all_Tsamples = general_CPA(known_input=known_input, hyp_leakage_cacl=HW_8bit_intermediateVal,\n",
    "                                    waves=waves, ith_weight=ith_weight, calc_p_value=calc_p_value)\n",
    "    return corr_all_Tsamples, p_val_all_Tsamples\n",
    "\n",
    "def HW_32bit_ANN_CPA(known_input : list[int], waves = np.array([]), n_traces: int = None, ith_weight: int = 0, calc_p_value: bool = False): # Vanilla ANN CPA, for one time_sample a\n",
    "    corr_all_Tsamples, p_val_all_Tsamples = general_CPA(known_input=known_input, hyp_leakage_cacl=HW_32bit_intermediateVal,\n",
    "                                    waves=waves, ith_weight=ith_weight, calc_p_value=calc_p_value)    \n",
    "    return corr_all_Tsamples, p_val_all_Tsamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056b128e-74d6-417d-a876-3796b20c9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Agregate Functions: using opensource libraries, such as scipy, or np for calculating correlation,\n",
    "# as oposed to above functions using specific functions for correlation calculations\n",
    "#*\n",
    "def corr_2TraceMatrixes(traces: np.ndarray, inputs: list[int],\n",
    "                        trace_len: int = 24000, n_traces: int = 1000,\n",
    "                        xth_secret_val: int = 0, secret_range: int = 256,\n",
    "                        corr_func: function = np.corrcoef): # function can be also sc.pearsonr\n",
    "  #hw = [hamming_weight(secret_value) for secret_value in range(secret_range)]\n",
    "  leak_model = np.array([[calc_intermediate_val(secret_val, inputs[j]) for j in range(n_traces)] for secret_val in range(secret_range)])\n",
    "  corr_all = []\n",
    "  for i in trange(secret_range, desc='Calculating Correlations for secret key'):\n",
    "    corr_curr = []\n",
    "    to_corr_array = leak_model[i, :]\n",
    "    for j in range(trace_len):\n",
    "      corr_curr.append( corr_func(traces[:,j], to_corr_array))\n",
    "    corr_all.append(corr_curr)\n",
    "  return np.array(corr_all)\n",
    "from typing import Callable as function\n",
    "import scipy.stats as sc\n",
    "def sc_pearson_2TraceMatrixes(traces: np.ndarray, inputs: list[int],\n",
    "                        trace_len: int = 24000, n_traces: int = 1000,\n",
    "                        xth_secret_val: int = 0, secret_range: int = 256): # function can be also sc.pearsonr\n",
    "  #hw = [hamming_weight(secret_value) for secret_value in range(secret_range)]\n",
    "  leak_model = np.array([[calc_intermediate_val(secret_val, inputs[j]) for j in range(n_traces)] for secret_val in range(secret_range)])\n",
    "  corr_all = []\n",
    "  for i in trange(secret_range, desc='Calculating Correlations for secret key'):\n",
    "    corr_curr = []\n",
    "    #to_corr_array = np.array([leak_model[i,:] for n in range(trace_len)]).transpose()\n",
    "    #corr_all.append(scipy.stats.pearsonr(traces[:,0:trace_len], to_corr_array, axis=0))\n",
    "    for j in range(trace_len):\n",
    "      corr_curr.append(sc.pearsonr(traces[:,j], leak_model[i,:]))\n",
    "    #corr_all.append(sc.pearsonr(traces[:,0:trace_len], to_corr_array, axis=0))\n",
    "    corr_all.append(corr_curr)\n",
    "  return np.array(corr_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23df889-5d54-4d94-a3ce-1fe70c61eb23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extract weights from correlation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b113b25a-a8d7-42bd-a163-a06262b51c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Agregate Functions: for extracting relevant peaks of correlation matrix, with several fitting parameters\n",
    "#*\n",
    "def get_weight(correlation_matrix: np.array, min_treshold = 0.0, peak_range = 0.3, show_info=False, lastIndex = 0, map_2dgrMaxs = None, mountain_half_dist = 20, level = 5):\n",
    "    #FIND index of highest value for each column (correlation of time sample)\n",
    "    indx_ColmnMax = correlation_matrix.argmax(axis=0) #Return an vector of indexes of max values in each column of matrix\n",
    "    \n",
    "    #print(f\"Input Vector: {correlation_matrix[1]}\")\n",
    "\n",
    "    #CREATE an array from previous finds\n",
    "    colmn_MaxVector = np.array([correlation_matrix[indx_ColmnMax[i]][i] for i in range(len(indx_ColmnMax))]) #Return a vector of max values for each column in matrix\n",
    "    for i in range(len(colmn_MaxVector)):\n",
    "        if colmn_MaxVector[i] < min_treshold:\n",
    "            colmn_MaxVector[i] = 0\n",
    "    #FIND GLOBAL MAX\n",
    "    indx_globMax = np.argmax(colmn_MaxVector)\n",
    "    val_globMax = colmn_MaxVector[indx_globMax]\n",
    "    vec_len = len(colmn_MaxVector)\n",
    "    #print(f\"Max Vector: {colmn_MaxVector}\")\n",
    "\n",
    "\n",
    "    if map_2dgrMaxs is None:\n",
    "        print(f\"Peak range: {peak_range}\")\n",
    "        print(f\"{len([i for i in colmn_MaxVector if i != 0])}\")\n",
    "        vals_locMaxVector0 = []\n",
    "        tmp_indices = []\n",
    "        for i in range(len(colmn_MaxVector)):\n",
    "            tmp_indices.append(i)\n",
    "            vals_locMaxVector0.append(0)\n",
    "        for i in range(level):\n",
    "            tmp_array = []\n",
    "            for indx in tmp_indices:\n",
    "                tmp_array.append(colmn_MaxVector[indx])\n",
    "            indices, props = scipy.signal.find_peaks(tmp_array)\n",
    "            for indx in range(len(indices)):\n",
    "                indices[indx] = tmp_indices[indices[indx]]\n",
    "            tmp_indices = indices\n",
    "        for indx in range(len(colmn_MaxVector)):\n",
    "            if val_globMax-peak_range <= colmn_MaxVector[indx]:\n",
    "                vals_locMaxVector0[indx] = colmn_MaxVector[indx]\n",
    "        \n",
    "        vals_locMaxVector1 = []\n",
    "        #FIND 1. local maxims and zero-out all values in tresshold range to global_correlation\n",
    "        for indx in range(vec_len): #shoud create vector with values which are local-maxims, other values are zeroed out\n",
    "            value = colmn_MaxVector[indx]\n",
    "            val_toAppend = 0\n",
    "            if (value + peak_range) >= val_globMax and ((indx+1) < vec_len) and (value > colmn_MaxVector[indx+1]):\n",
    "                val_toAppend = value\n",
    "            vals_locMaxVector1.append(val_toAppend)\n",
    "        secret_corr = 0 #FINAL correlation\n",
    "        secret_colmn = 0 #Column/Time sample of Final correlation\n",
    "    \n",
    "        #FIND 2. local maxims\n",
    "        vals_locMaxVector2 = []\n",
    "        last_val = vals_locMaxVector1[0]\n",
    "        last_indx = 0\n",
    "        zero_counter = 0\n",
    "        for indx in range(vec_len): #shoud create vector with values which are local-maxims, other values are zeroed out\n",
    "            value = vals_locMaxVector1[indx]\n",
    "            vals_locMaxVector2.append(0)\n",
    "            if value != 0:#Cross the desert, and on start of the next mountain, reminiscence about last mountain and save it to the memoar \n",
    "                if zero_counter > mountain_half_dist:\n",
    "                    vals_locMaxVector2[last_indx] = last_val\n",
    "                    #Reset\n",
    "                    last_val = 0\n",
    "                    last_indx = indx\n",
    "                else:\n",
    "                    if last_val < value:\n",
    "                        last_val = value\n",
    "                        last_indx = indx\n",
    "                zero_counter = 0\n",
    "            else:\n",
    "                zero_counter += 1\n",
    "        vals_locMaxVector2[last_indx] = last_val\n",
    "\n",
    "\n",
    "\n",
    "        #FIND 3. local maxims\n",
    "        vals_locMaxVector3 = []\n",
    "        last_val = vals_locMaxVector1[0]\n",
    "        last_indx = 0\n",
    "        zero_counter = 0\n",
    "        for indx in range(vec_len): #shoud create vector with values which are local-maxims, other values are zeroed out\n",
    "            value = vals_locMaxVector1[indx]\n",
    "            vals_locMaxVector3.append(0)\n",
    "            if value > (last_val-(peak_range/2)):#Cross the desert, and on start of the next mountain, reminiscence about last mountain and save it to the memoar \n",
    "                if zero_counter > mountain_half_dist:\n",
    "                    vals_locMaxVector3[last_indx] = last_val\n",
    "                    #Reset\n",
    "                    last_val = 0\n",
    "                    last_indx = indx\n",
    "                else:\n",
    "                    if  last_val > value:\n",
    "                        vals_locMaxVector3[last_indx] = last_val\n",
    "                    last_val = value\n",
    "                    last_indx = indx\n",
    "                zero_counter = 0\n",
    "            else:\n",
    "                zero_counter += 1\n",
    "        vals_locMaxVector3[last_indx] = last_val\n",
    "\n",
    "\n",
    "        if show_info:    \n",
    "            print(f\" Index of global max: {indx_globMax} and global_max_val: {val_globMax}\")\n",
    "            show_extraction_method_info(vals_locMaxVector1, indx_ColmnMax, method_name = f'1th degree', color = \"red\")\n",
    "            show_extraction_method_info(vals_locMaxVector0, indx_ColmnMax, method_name = f'{level}-level', color = \"green\")\n",
    "            show_extraction_method_info(vals_locMaxVector2, indx_ColmnMax, method_name = f'2th-var degree', color = \"blue\")\n",
    "            show_extraction_method_info(vals_locMaxVector3, indx_ColmnMax, method_name = f'3th-var degree', color = \"purple\")\n",
    "    else:\n",
    "        vals_locMaxVector2 = map_2dgrMaxs\n",
    "    \n",
    "    secret_corr = 0 #FINAL correlation\n",
    "    secret_colmn = 0 #Column/Time sample of Final correlation\n",
    "    nextIndex = lastIndex\n",
    "    found = False\n",
    "    print(f\"In range {lastIndex} - {len(indx_ColmnMax)}\")\n",
    "    for indx in range(lastIndex, len(indx_ColmnMax)): #Find first local maxim in the peak_range from global maxim\n",
    "        value = vals_locMaxVector2[indx]\n",
    "        if value != 0:\n",
    "            if not found:\n",
    "                secret_corr = value\n",
    "                secret_colmn = indx\n",
    "                found = True\n",
    "                continue\n",
    "            nextIndex =  indx - round((indx - secret_colmn) / 2)\n",
    "            break\n",
    "    secret_val = indx_ColmnMax[secret_colmn]\n",
    "\n",
    "    \n",
    "    print(f\" Found the soonest secret value is {secret_val}, at the time sample {secret_colmn}, with correlation {secret_corr = :.3f}\")\n",
    "    return secret_val, secret_colmn, secret_corr, vals_locMaxVector2, colmn_MaxVector, nextIndex, vals_locMaxVector2\n",
    "#results_directory = {}\n",
    "#secret_value, time_sample, correlation, localMax_vector = get_weight(np.array(ncorr_all), show_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf8728-0782-49b9-ab2e-ae35abb3daab",
   "metadata": {},
   "source": [
    "## Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d45b8f2-badb-4975-a84f-795886cf2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ The Analyser succesfuly runned.\n"
     ]
    }
   ],
   "source": [
    "print(\"✔️ The Analyser succesfuly runned.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
