{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ba739e-b1e3-4824-bdb0-a7514253e664",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2c072-81cf-47c6-9ef5-d9a4c19114cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calculate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e55c142-1a9f-4a66-b11d-f2c57bc0d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644f0831-837a-43fe-ac53-378f0b8c08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnum = 8\n",
    "corr_all_Tsamples = []\n",
    "secret_range = 2**(bnum)\n",
    "M = round(secret_range / 2)\n",
    "#hw = [hamming_weight(secret_value) for secret_value in range(secret_range)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f85d97a-f7a9-4942-9bdc-14343e391c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "mult_result = (256 * 10) % secret_range\n",
    "#print(hw[mult_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869f1b3b-1ed2-4e86-b3cb-a07f5c85d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ulity functions\n",
    "def plot_mult(corr_list, _type = \"norm\", mult=True, xlim:list[int]=None, ylim:list[int]=None, graph_title=\"Multiple Correlation Traces\", ylabel=\"Correlation\"):\n",
    "  if not mult: #len(corr_list.shape) >  1:\n",
    "    corr_list = [corr_list]\n",
    "  image = plt.figure()\n",
    "  plt.title(graph_title)\n",
    "  plt.xlabel(\"Index\")\n",
    "  plt.ylabel(ylabel)\n",
    "  if xlim is not None:\n",
    "      plt.xlim(xlim)\n",
    "  if ylim is not None:\n",
    "      plt.ylim(ylim)\n",
    "  for corr in corr_list:\n",
    "      plt.plot(corr)#, label=f'Trace {i+1}')\n",
    "  plt.show()\n",
    "  os.makedirs('./figures', exist_ok=True)\n",
    "  image.savefig(f'./figures/{_type}_corr.png')\n",
    "  plt.close()  # Close the figure to free memory\n",
    "def createDiffWave(waves, name):\n",
    "        diff_waves = []\n",
    "        square_waves = []\n",
    "        avg_wave = np.mean(waves,axis=0)\n",
    "        \n",
    "        for i in waves:\n",
    "            diff_waves.append(np.subtract(i,avg_wave))\n",
    "        for i in diff_waves:\n",
    "            square_wave = []\n",
    "            for ii in i:\n",
    "                square_wave.append(ii**2)\n",
    "            square_waves.append(square_wave)\n",
    "        \n",
    "        diff_avg = np.mean(diff_waves,axis=0)\n",
    "        var = np.mean(square_waves,axis=0)\n",
    "    \n",
    "        plot_mult(np.array(waves[0]), _type = f\"wave0_{name}\", mult=False)\n",
    "        plot_mult(np.array(avg_wave), _type = f\"avg_wave_{name}\", mult=False)\n",
    "        plot_mult(np.array(diff_avg), _type = f\"difference_wave_{name}\", mult=False)\n",
    "        plot_mult(np.array(var), _type = f\"variation_{name}\", mult=False)\n",
    "        return diff_avg, avg_wave, var, waves\n",
    "def get_overflowed_val(val = 0, bnum = 8):\n",
    "    border = 2**(bnum-1)\n",
    "    if val >= border:\n",
    "        val = val % (border*2)\n",
    "        if val >= border:\n",
    "            val = -border + (val - border)\n",
    "    return val\n",
    "def hamming_weight(x, is_int = True):\n",
    "    if not is_int:\n",
    "        return np.count_nonzero(x == 1)\n",
    "    return bin(x).count(\"1\")\n",
    "def calc_p_val(corr: float, set_len: int, mode: int = 0) -> float:\n",
    "  t_stat = corr*( ((set_len-2) / (1 - corr**2))**0.5 )\n",
    "  if mode == 0:\n",
    "    df = set_len - 2  # degrees of freedom\n",
    "    p_value = 2 * stats.t.sf(abs(t_stat), df)  # Two-tailed p-value\n",
    "    return p_value\n",
    "def calc_intermediate_val(secret_value: int, knonw_input: int):\n",
    "    #return hamming_weight(np.uint32(np.uint8(secret_value) * np.uint8(knonw_input)))\n",
    "    return hamming_weight(np.uint32(secret_value) * np.uint32(knonw_input))\n",
    "def calc_intermediate_val2(secret_value: int, knonw_input: int):\n",
    "    #return hamming_weight(np.uint32(np.uint8(secret_value) * np.uint8(knonw_input)) % 256)\n",
    "    return hamming_weight(np.uint32(secret_value) * np.uint32(knonw_input) % 256)\n",
    "def calc_intermediate_val3(secret_value: int, knonw_input: int):\n",
    "    #return hamming_weight(np.uint32(np.uint8(secret_value) * np.uint8(knonw_input)) % 256)\n",
    "    return np.uint32(secret_value) * np.uint32(knonw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61e7978-068f-407c-a639-64fc29100f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_weight(np.uint32(1023 * 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a58495-c1c5-4767-8de0-4adf2dfa3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_weight(np.uint8(255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c06c8c35-30e4-4d57-baf6-233db1a37abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_weight(np.uint8(255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "297d647c-2187-4c67-b76e-9593882ce0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28050"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "255 * 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37870ad6-8f87-4ddd-8ab3-b826fa16b146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_intermediate_val2(1023, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c445d8-01c9-4933-91ad-4224223c71e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_intermediate_val(1023, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9ea799-bd6a-43cb-83be-518f2c50f57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_intermediate_val(255, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "667568ec-7736-4179-bbe0-c8e1f5cdc9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregate Functions: For calculation of correlation above, waves dataset, with hypotetical created traces based of known_input\n",
    "def ancientV2_ANN_CPA_OTS(known_input : list[int], waves = np.array([]), n_traces: int = None, trace_len: int = None, ith_weight: int = 0, calc_p_value: bool = False): # Vanilla ANN CPA, for one time_sample a\n",
    "    # Verification of parameters:\n",
    "    if n_traces is None:\n",
    "      n_traces = len(known_input)\n",
    "    if trace_len is None:\n",
    "      trace_len = len(waves[0])\n",
    "\n",
    "    # Preparational calculations\n",
    "    bnum = 8\n",
    "    corr_all_Tsamples = []\n",
    "    secret_range = 2**(bnum)\n",
    "    #hw = [hamming_weight(secret_value) for secret_value in range(secret_range)]\n",
    "\n",
    "    # Calculating statistics for traces in waves set\n",
    "    qsum_L_list = []\n",
    "    l_diff_list = []\n",
    "    for time_sample in range(trace_len):\n",
    "      # Calculation preparations: target sets extraction\n",
    "      L = waves[:,time_sample]\n",
    "      L_mean = np.mean(L)\n",
    "      # Calculation\n",
    "      l_diff = L - L_mean  # Vectorized\n",
    "      qsum_L = np.sum(np.square(l_diff))  # Sum of squared differences\n",
    "      # Tidy up\n",
    "      qsum_L_list.append(qsum_L)\n",
    "      l_diff_list.append(l_diff)\n",
    "\n",
    "    for secret_value in trange(secret_range, desc='Calculating Correlations for the Secret-Key: '): # For current WeightHypothesis do\n",
    "        corr_Tsamples = []\n",
    "        #H = [calc_intermediate_val3(secret_value, known_input[j]) for j in range(n_traces)]\n",
    "        H = np.uint32(known_input *  secret_value)\n",
    "        H_mean = np.mean(H)\n",
    "        h_diff = np.array(H) - H_mean\n",
    "        qsum_H = np.sum(np.square(h_diff))\n",
    "\n",
    "        # For current WeightHypothesis, create an Correlation, this vector needs to be created for each time sample:\n",
    "        for time_sample in range(trace_len):\n",
    "            # Calculation preparations: target sets extraction\n",
    "            L = waves[:,time_sample]\n",
    "            l_diff = l_diff_list[time_sample]\n",
    "            sum_HL = np.sum(h_diff * l_diff)  # Dot product\n",
    "            # Calculation\n",
    "            divider = (qsum_H ** 0.5) * (qsum_L_list[time_sample] ** 0.5)\n",
    "            corr = sum_HL / divider if divider != 0 else 0\n",
    "            if calc_p_value:\n",
    "              t_stat = corr*( ((n_traces-2) / (1 - corr**2))**0.5 )\n",
    "              df = n_traces - 2  # degrees of freedom\n",
    "              p_value = 2 * stats.t.sf(abs(t_stat), df)  # Two-tailed p-value\n",
    "              corr = [corr, p_value]\n",
    "            # Tidy up\n",
    "            corr_Tsamples.append(corr)\n",
    "        corr_all_Tsamples.append(corr_Tsamples)\n",
    "    return np.array(corr_all_Tsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f60439c-a275-4a79-b4f1-4b63c561849d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  2  6  4  4  6 10  6]\n",
      "[256, 258, 4550, 1666, 2245, 6546, 7888, 6788]\n",
      "[  512   516  9100  3332  4490 13092 15776 13576]\n",
      "[ 256  258 4550 1666 2245 6546 7888 6788]\n",
      "256\n",
      "258\n",
      "4550\n",
      "1666\n",
      "2245\n",
      "6546\n",
      "7888\n",
      "6788\n"
     ]
    }
   ],
   "source": [
    "temp = [256,258,4550,1666,2245,6546,7888,6788]\n",
    "tmp_a = np.array(temp)\n",
    "\n",
    "a = np.uint32(tmp_a - 1)\n",
    "a = np.array([bin(i).count(\"1\") for i in a])\n",
    "print(a)\n",
    "print(temp)\n",
    "HH = np.uint16(tmp_a *  2)\n",
    "print(HH)\n",
    "print(np.array(temp))\n",
    "for i in np.array(temp):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56caad9f-2716-45f2-94ab-3f58872b09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnum = 16\n",
    "secret_range = 2**(bnum)\n",
    "hw = [hamming_weight(secret_value) for secret_value in range(secret_range)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce4a373d-ab25-47c5-b2ef-b2fc077ed62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregate Functions: For calculation of correlation above, waves dataset, with hypotetical created traces based of known_input\n",
    "def oldV2_ANN_CPA_OTS(known_input : list[int], waves = np.array([]), n_traces: int = None, trace_len: int = None, ith_weight: int = 0, calc_p_value: bool = False): # Vanilla ANN CPA, for one time_sample a\n",
    "    # Verification of parameters:\n",
    "    if n_traces is None:\n",
    "      n_traces = len(known_input)\n",
    "    if trace_len is None:\n",
    "      trace_len = len(waves[0])\n",
    "\n",
    "    # Preparational calculations\n",
    "    bnum = 8\n",
    "    corr_all_Tsamples = []\n",
    "    secret_range = 2**(bnum)\n",
    "    global hw\n",
    "\n",
    "    # Calculating statistics for traces in waves set\n",
    "    qsum_L_list = []\n",
    "    l_diff_list = []\n",
    "    for time_sample in range(trace_len):\n",
    "      # Calculation preparations: target sets extraction\n",
    "      L = waves[:,time_sample]\n",
    "      L_mean = np.mean(L)\n",
    "      # Calculation\n",
    "      l_diff = L - L_mean  # Vectorized\n",
    "      qsum_L = np.sum(np.square(l_diff))  # Sum of squared differences\n",
    "      # Tidy up\n",
    "      qsum_L_list.append(qsum_L)\n",
    "      l_diff_list.append(l_diff)\n",
    "\n",
    "    for secret_value in trange(secret_range, desc='Calculating Correlations for the Secret-Key: '): # For current WeightHypothesis do\n",
    "        corr_Tsamples = []\n",
    "        H = [hw[np.uint32(known_input[i] *  secret_value) % 256] for i in range(n_traces)]\n",
    "        H_mean = np.mean(H)\n",
    "        h_diff = H - H_mean\n",
    "        qsum_H = np.sum(np.square(h_diff))\n",
    "\n",
    "        # For current WeightHypothesis, create an Correlation, this vector needs to be created for each time sample:\n",
    "        for time_sample in range(trace_len):\n",
    "            # Calculation preparations: target sets extraction\n",
    "            L = waves[:,time_sample]\n",
    "            l_diff = l_diff_list[time_sample]\n",
    "            sum_HL = np.sum(h_diff * l_diff)  # Dot product\n",
    "            # Calculation\n",
    "            divider = (qsum_H ** 0.5) * (qsum_L_list[time_sample] ** 0.5)\n",
    "            corr = sum_HL / divider if divider != 0 else 0\n",
    "            if calc_p_value:\n",
    "              t_stat = corr*( ((n_traces-2) / (1 - corr**2))**0.5 )\n",
    "              df = n_traces - 2  # degrees of freedom\n",
    "              p_value = 2 * stats.t.sf(abs(t_stat), df)  # Two-tailed p-value\n",
    "              corr = [corr, p_value]\n",
    "            # Tidy up\n",
    "            corr_Tsamples.append(corr)\n",
    "        corr_all_Tsamples.append(corr_Tsamples)\n",
    "    return np.array(corr_all_Tsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38c8d4b9-d54e-4af5-9223-2050221e45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregate Functions: For calculation of correlation above, waves dataset, with hypotetical created traces based of known_input\n",
    "def V2_ANN_CPA_OTS(known_input : list[int], waves = np.array([]), n_traces: int = None, trace_len: int = None, ith_weight: int = 0, calc_p_value: bool = False): # Vanilla ANN CPA, for one time_sample a\n",
    "    # Verification of parameters:\n",
    "    if n_traces is None:\n",
    "      n_traces = len(known_input)\n",
    "    if trace_len is None:\n",
    "      trace_len = len(waves[0])\n",
    "\n",
    "    # Preparational calculations\n",
    "    bnum = 8\n",
    "    corr_all_Tsamples = []\n",
    "    secret_range = 2**(bnum)\n",
    "    global hw\n",
    "\n",
    "    # Calculating statistics for traces in waves set\n",
    "    qsum_L_list = []\n",
    "    l_diff_list = []\n",
    "    for time_sample in range(trace_len):\n",
    "      # Calculation preparations: target sets extraction\n",
    "      L = waves[:,time_sample]\n",
    "      L_mean = np.mean(L)\n",
    "      # Calculation\n",
    "      l_diff = L - L_mean  # Vectorized\n",
    "      qsum_L = np.sum(np.square(l_diff))  # Sum of squared differences\n",
    "      # Tidy up\n",
    "      qsum_L_list.append(qsum_L)\n",
    "      l_diff_list.append(l_diff)\n",
    "\n",
    "    for secret_value in trange(secret_range, desc='Calculating Correlations for the Secret-Key: '): # For current WeightHypothesis do\n",
    "        corr_Tsamples = []\n",
    "        H = [hw[np.uint32(known_input[i] *  secret_value)] for i in range(n_traces)]\n",
    "        H_mean = np.mean(H)\n",
    "        h_diff = H - H_mean\n",
    "        qsum_H = np.sum(np.square(h_diff))\n",
    "\n",
    "        # For current WeightHypothesis, create an Correlation, this vector needs to be created for each time sample:\n",
    "        for time_sample in range(trace_len):\n",
    "            # Calculation preparations: target sets extraction\n",
    "            L = waves[:,time_sample]\n",
    "            l_diff = l_diff_list[time_sample]\n",
    "            sum_HL = np.sum(h_diff * l_diff)  # Dot product\n",
    "            # Calculation\n",
    "            divider = (qsum_H ** 0.5) * (qsum_L_list[time_sample] ** 0.5)\n",
    "            corr = sum_HL / divider if divider != 0 else 0\n",
    "            if calc_p_value:\n",
    "              t_stat = corr*( ((n_traces-2) / (1 - corr**2))**0.5 )\n",
    "              df = n_traces - 2  # degrees of freedom\n",
    "              p_value = 2 * stats.t.sf(abs(t_stat), df)  # Two-tailed p-value\n",
    "              corr = [corr, p_value]\n",
    "            # Tidy up\n",
    "            corr_Tsamples.append(corr)\n",
    "        corr_all_Tsamples.append(corr_Tsamples)\n",
    "    return np.array(corr_all_Tsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056b128e-74d6-417d-a876-3796b20c9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable as function\n",
    "def corr_2TraceMatrixes(traces: np.ndarray, inputs: list[int],\n",
    "                        trace_len: int = 24000, n_traces: int = 1000,\n",
    "                        xth_secret_val: int = 0, secret_range: int = 256,\n",
    "                        corr_func: function = np.corrcoef): # function can be also sc.pearsonr\n",
    "  #hw = [hamming_weight(secret_value) for secret_value in range(secret_range)]\n",
    "  leak_model = np.array([[calc_intermediate_val(secret_val, inputs[j]) for j in range(n_traces)] for secret_val in range(secret_range)])\n",
    "  corr_all = []\n",
    "  for i in trange(secret_range, desc='Calculating Correlations for secret key'):\n",
    "    corr_curr = []\n",
    "    to_corr_array = leak_model[i, :]\n",
    "    for j in range(trace_len):\n",
    "      corr_curr.append( corr_func(traces[:,j], to_corr_array))\n",
    "    corr_all.append(corr_curr)\n",
    "  return np.array(corr_all)\n",
    "from typing import Callable as function\n",
    "import scipy.stats as sc\n",
    "def sc_pearson_2TraceMatrixes(traces: np.ndarray, inputs: list[int],\n",
    "                        trace_len: int = 24000, n_traces: int = 1000,\n",
    "                        xth_secret_val: int = 0, secret_range: int = 256): # function can be also sc.pearsonr\n",
    "  #hw = [hamming_weight(secret_value) for secret_value in range(secret_range)]\n",
    "  leak_model = np.array([[calc_intermediate_val(secret_val, inputs[j]) for j in range(n_traces)] for secret_val in range(secret_range)])\n",
    "  corr_all = []\n",
    "  for i in trange(secret_range, desc='Calculating Correlations for secret key'):\n",
    "    corr_curr = []\n",
    "    #to_corr_array = np.array([leak_model[i,:] for n in range(trace_len)]).transpose()\n",
    "    #corr_all.append(scipy.stats.pearsonr(traces[:,0:trace_len], to_corr_array, axis=0))\n",
    "    for j in range(trace_len):\n",
    "      corr_curr.append(sc.pearsonr(traces[:,j], leak_model[i,:]))\n",
    "    #corr_all.append(sc.pearsonr(traces[:,0:trace_len], to_corr_array, axis=0))\n",
    "    corr_all.append(corr_curr)\n",
    "  return np.array(corr_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23df889-5d54-4d94-a3ce-1fe70c61eb23",
   "metadata": {},
   "source": [
    "## Extract weights from correlation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66cd12a0-fa53-4d48-b1a1-c1fa02b80433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextColor():\n",
    "        black = '\\033[30m'\n",
    "        red = '\\033[31m'\n",
    "        green = '\\033[32m'\n",
    "        orange = '\\033[33m'\n",
    "        blue = '\\033[34m'\n",
    "        purple = '\\033[35m'\n",
    "        cyan = '\\033[36m'\n",
    "        lightgrey = '\\033[37m'\n",
    "        darkgrey = '\\033[90m'\n",
    "        lightred = '\\033[91m'\n",
    "        lightgreen = '\\033[92m'\n",
    "        yellow = '\\033[93m'\n",
    "        lightblue = '\\033[94m'\n",
    "        pink = '\\033[95m'\n",
    "        lightcyan = '\\033[96m'\n",
    "        reset = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55908a4a-91ca-465b-95d7-68d6133d25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_extraction_method_info(vals_locMaxVector: list[int], indx_ColmnMax: list[int], method_name: str = f'xth-level degree', color: str = \"blue\"):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(vals_locMaxVector, color=color)\n",
    "    plt.title(f'{method_name} Local maxims graph')\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"Secret Value\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    indx_localMax = []\n",
    "    for i in range(len(vals_locMaxVector)):\n",
    "        val_toAppend = 0\n",
    "        if vals_locMaxVector[i] > 0:\n",
    "            val_toAppend = indx_ColmnMax[i]\n",
    "        indx_localMax.append(val_toAppend)\n",
    "    local_maxims = [int(indx_ColmnMax[i]) for i in range(len(indx_ColmnMax)) if vals_locMaxVector[i] > 0]\n",
    "    uniques_set = [ x for i, x in enumerate(local_maxims) if x not in local_maxims[:i]]\n",
    "    print(f\"Number of uniques: {len(uniques_set)}\")\n",
    "    print(f\"Number of local maxims: {len(local_maxims)}\")\n",
    "    print(f\"Uniquess: {TextColor.pink}{uniques_set}{TextColor.reset}\")\n",
    "    print(f\"Local maxims: {TextColor.orange}{local_maxims}{TextColor.reset}\")\n",
    "    fig = plt.figure()\n",
    "    plt.plot(indx_localMax, color=color)\n",
    "    plt.title(f\"{method_name} indexes graph\")\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"Index\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b113b25a-a8d7-42bd-a163-a06262b51c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(correlation_matrix: np.array, min_treshold = 0.0, peak_range = 0.3, show_info=False, lastIndex = 0, map_2dgrMaxs = None, mountain_half_dist = 20, level = 5):\n",
    "    #FIND index of highest value for each column (correlation of time sample)\n",
    "    indx_ColmnMax = correlation_matrix.argmax(axis=0) #Return an vector of indexes of max values in each column of matrix\n",
    "    \n",
    "    #print(f\"Input Vector: {correlation_matrix[1]}\")\n",
    "\n",
    "    #CREATE an array from previous finds\n",
    "    colmn_MaxVector = np.array([correlation_matrix[indx_ColmnMax[i]][i] for i in range(len(indx_ColmnMax))]) #Return a vector of max values for each column in matrix\n",
    "    for i in range(len(colmn_MaxVector)):\n",
    "        if colmn_MaxVector[i] < min_treshold:\n",
    "            colmn_MaxVector[i] = 0\n",
    "    #FIND GLOBAL MAX\n",
    "    indx_globMax = np.argmax(colmn_MaxVector)\n",
    "    val_globMax = colmn_MaxVector[indx_globMax]\n",
    "    vec_len = len(colmn_MaxVector)\n",
    "    #print(f\"Max Vector: {colmn_MaxVector}\")\n",
    "\n",
    "\n",
    "    if map_2dgrMaxs is None:\n",
    "        print(f\"{len([i for i in colmn_MaxVector if i != 0])}\")\n",
    "        vals_locMaxVector0 = []\n",
    "        tmp_indices = []\n",
    "        for i in range(len(colmn_MaxVector)):\n",
    "            tmp_indices.append(i)\n",
    "            vals_locMaxVector0.append(0)\n",
    "        for i in range(level):\n",
    "            tmp_array = []\n",
    "            for indx in tmp_indices:\n",
    "                tmp_array.append(colmn_MaxVector[indx])\n",
    "            indices, props = scipy.signal.find_peaks(tmp_array)\n",
    "            for indx in range(len(indices)):\n",
    "                indices[indx] = tmp_indices[indices[indx]]\n",
    "            tmp_indices = indices\n",
    "        for indx in tmp_indices:\n",
    "            if val_globMax-peak_range <= colmn_MaxVector[indx]:\n",
    "                vals_locMaxVector0[indx] = colmn_MaxVector[indx]\n",
    "        \n",
    "        vals_locMaxVector1 = []\n",
    "        #FIND 1. local maxims and zero-out all values in tresshold range to global_correlation\n",
    "        for indx in range(vec_len): #shoud create vector with values which are local-maxims, other values are zeroed out\n",
    "            value = colmn_MaxVector[indx]\n",
    "            val_toAppend = 0\n",
    "            if (value + peak_range) >= val_globMax and ((indx+1) < vec_len) and (value > colmn_MaxVector[indx+1]):\n",
    "                val_toAppend = value\n",
    "            vals_locMaxVector1.append(val_toAppend)\n",
    "        secret_corr = 0 #FINAL correlation\n",
    "        secret_colmn = 0 #Column/Time sample of Final correlation\n",
    "    \n",
    "        #FIND 2. local maxims\n",
    "        vals_locMaxVector2 = []\n",
    "        last_val = vals_locMaxVector1[0]\n",
    "        last_indx = 0\n",
    "        zero_counter = 0\n",
    "        for indx in range(vec_len): #shoud create vector with values which are local-maxims, other values are zeroed out\n",
    "            value = vals_locMaxVector1[indx]\n",
    "            vals_locMaxVector2.append(0)\n",
    "            if value != 0:#Cross the desert, and on start of the next mountain, reminiscence about last mountain and save it to the memoar \n",
    "                if zero_counter > mountain_half_dist:\n",
    "                    vals_locMaxVector2[last_indx] = last_val\n",
    "                    #Reset\n",
    "                    last_val = 0\n",
    "                    last_indx = indx\n",
    "                else:\n",
    "                    if last_val < value:\n",
    "                        last_val = value\n",
    "                        last_indx = indx\n",
    "                zero_counter = 0\n",
    "            else:\n",
    "                zero_counter += 1\n",
    "        vals_locMaxVector2[last_indx] = last_val\n",
    "\n",
    "\n",
    "\n",
    "        #FIND 3. local maxims\n",
    "        vals_locMaxVector3 = []\n",
    "        last_val = vals_locMaxVector1[0]\n",
    "        last_indx = 0\n",
    "        zero_counter = 0\n",
    "        for indx in range(vec_len): #shoud create vector with values which are local-maxims, other values are zeroed out\n",
    "            value = vals_locMaxVector1[indx]\n",
    "            vals_locMaxVector3.append(0)\n",
    "            if value > (last_val-(peak_range/2)):#Cross the desert, and on start of the next mountain, reminiscence about last mountain and save it to the memoar \n",
    "                if zero_counter > mountain_half_dist:\n",
    "                    vals_locMaxVector3[last_indx] = last_val\n",
    "                    #Reset\n",
    "                    last_val = 0\n",
    "                    last_indx = indx\n",
    "                else:\n",
    "                    if  last_val > value:\n",
    "                        vals_locMaxVector3[last_indx] = last_val\n",
    "                    last_val = value\n",
    "                    last_indx = indx\n",
    "                zero_counter = 0\n",
    "            else:\n",
    "                zero_counter += 1\n",
    "        vals_locMaxVector3[last_indx] = last_val\n",
    "\n",
    "\n",
    "        if show_info:    \n",
    "            print(f\" Index of global max: {indx_globMax} and global_max_val: {val_globMax}\")\n",
    "            show_extraction_method_info(vals_locMaxVector1, indx_ColmnMax, method_name = f'1th degree', color = \"red\")\n",
    "            show_extraction_method_info(vals_locMaxVector0, indx_ColmnMax, method_name = f'{level}-level', color = \"green\")\n",
    "            show_extraction_method_info(vals_locMaxVector2, indx_ColmnMax, method_name = f'2th-var degree', color = \"blue\")\n",
    "            show_extraction_method_info(vals_locMaxVector3, indx_ColmnMax, method_name = f'3th-var degree', color = \"purple\")\n",
    "    else:\n",
    "        vals_locMaxVector2 = map_2dgrMaxs\n",
    "    \n",
    "    secret_corr = 0 #FINAL correlation\n",
    "    secret_colmn = 0 #Column/Time sample of Final correlation\n",
    "    nextIndex = lastIndex\n",
    "    found = False\n",
    "    print(f\"In range {lastIndex} - {len(indx_ColmnMax)}\")\n",
    "    for indx in range(lastIndex, len(indx_ColmnMax)): #Find first local maxim in the peak_range from global maxim\n",
    "        value = vals_locMaxVector2[indx]\n",
    "        if value != 0:\n",
    "            if not found:\n",
    "                secret_corr = value\n",
    "                secret_colmn = indx\n",
    "                found = True\n",
    "                continue\n",
    "            nextIndex =  indx - round((indx - secret_colmn) / 2)\n",
    "            break\n",
    "    secret_val = indx_ColmnMax[secret_colmn]\n",
    "\n",
    "    \n",
    "    print(f\" Found the soonest secret value is {secret_val}, at the time sample {secret_colmn}, with correlation {secret_corr = :.3f}\")\n",
    "    return secret_val, secret_colmn, secret_corr, vals_locMaxVector2, colmn_MaxVector, nextIndex, vals_locMaxVector2\n",
    "#results_directory = {}\n",
    "#secret_value, time_sample, correlation, localMax_vector = get_weight(np.array(ncorr_all), show_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d45b8f2-badb-4975-a84f-795886cf2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ The Analyser succesfuly runned.\n"
     ]
    }
   ],
   "source": [
    "print(\"✔️ The Analyser succesfuly runned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d3733-f454-4217-aab3-afbe0a743c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
